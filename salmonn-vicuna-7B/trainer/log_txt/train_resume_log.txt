(KJPark) root@nota-gpu-svr009:/data/KJPark/audiolm-evaluator/audiolm-trainer# torchrun --nproc_per_node=2 train.py --cfg-path configs/train_stage2.yaml --resume_checkpoint outputs_stage2/202501300234/checkpoint_7.pth
W0130 14:57:51.170242 3657004 site-packages/torch/distributed/run.py:793] 
W0130 14:57:51.170242 3657004 site-packages/torch/distributed/run.py:793] *****************************************
W0130 14:57:51.170242 3657004 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0130 14:57:51.170242 3657004 site-packages/torch/distributed/run.py:793] *****************************************
| distributed init (rank 0, world 2): env://
[rank0]:[W130 14:57:56.290638145 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
| distributed init (rank 1, world 2): env://
[rank1]:[W130 14:57:56.294924857 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
10

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: sinseunghun708 (sinseunghun708-student). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/KJPark/audiolm-evaluator/audiolm-trainer/wandb/run-20250130_145758-v0aafopz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vicuna-7B-stage2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sinseunghun708-student/audio_lm
wandb: üöÄ View run at https://wandb.ai/sinseunghun708-student/audio_lm/runs/v0aafopz
2025-01-30 14:57:58,825 [INFO] 
=====  Running Parameters    =====
2025-01-30 14:57:58,826 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "vicuna-7B-stage2",
    "gpu": 0,
    "iters_per_epoch": 3000,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage2",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-30 14:57:58,826 [INFO] 
======  Dataset Attributes  ======
2025-01-30 14:57:58,826 [INFO] {
    "prefix": "/data/data",
    "train_ann_path": "/data/data/stage2/stage2_train_modified.json",
    "valid_ann_path": "/data/data/stage2/stage2_valid_modified.json",
    "whisper_path": "openai/whisper-large-v2"
}
2025-01-30 14:57:58,826 [INFO] 
======  Model Attributes  ======
2025-01-30 14:57:58,827 [INFO] {
    "beats_path": "/data/baseline/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/nathan/audiolm-evaluator/audiolm-trainer/outputs_stage1_only/202501272256/checkpoint_best.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "lmsys/vicuna-7b-v1.5",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "openai/whisper-large-v2",
    "window_level_Qformer": true
}
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 1/2 [00:00<00:00,  2.57it/s]2025-01-30 14:58:02,602 [INFO] Loading LLaMA Tokenizer
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.16it/s]
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
2025-01-30 14:58:03,120 [INFO] Loading LLaMA Model
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.09it/s]
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2025-01-30 15:00:23,359 [INFO] Loading LLaMA Done
trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525
2025-01-30 15:00:40,992 [INFO] LoRA Training
2025-01-30 15:00:40,992 [INFO] Loading Whisper Model
2025-01-30 15:00:42,125 [INFO] freeze Whisper
2025-01-30 15:00:42,125 [INFO] Loading BEATs Model
2025-01-30 15:00:42,450 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-30 15:00:45,216 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-30 15:00:49,881 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-30 15:00:49,904 [INFO] Load SALMONN ckpt from: /data/nathan/audiolm-evaluator/audiolm-trainer/outputs_stage1_only/202501272256/checkpoint_best.pth
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
GPU Memory Before Loading:
Allocated: 15925.00 MB
Cached: 31234.00 MB
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-30 15:00:56,543 [INFO] number of trainable parameters: 30186240
GPU Memory Before Loading:
Allocated: 15925.00 MB
Cached: 31234.00 MB
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location='cuda')
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location='cuda')
2025-01-30 15:00:59,270 [INFO] Training Phase
2025-01-30 15:00:59,279 [INFO] Start training epoch 7, 3000 iters per inner epoch.
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [7]  [   0/3000]  eta: 2:23:34  lr: 0.000027  loss: 0.0832  time: 2.8714  data: 0.0000  max mem: 25386
Train: data epoch: [7]  [   0/3000]  eta: 2:24:22  lr: 0.000027  loss: 0.0332  time: 2.8874  data: 0.0000  max mem: 20142
Train: data epoch: [7]  [   5/3000]  eta: 1:13:34  lr: 0.000027  loss: 0.1496  time: 1.4739  data: 0.0000  max mem: 25962
Train: data epoch: [7]  [   5/3000]  eta: 1:13:32  lr: 0.000027  loss: 0.0423  time: 1.4734  data: 0.0000  max mem: 25455
Train: data epoch: [7]  [  10/3000]  eta: 1:05:12  lr: 0.000027  loss: 1.0790  time: 1.3086  data: 0.0000  max mem: 25962
Train: data epoch: [7]  [  10/3000]  eta: 1:05:10  lr: 0.000027  loss: 0.1125  time: 1.3080  data: 0.0000  max mem: 25455