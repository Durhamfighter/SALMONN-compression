(KJPark) root@nota-gpu-svr009:/data/KJPark/audiolm-evaluator/audiolm-trainer# torchrun --nproc_per_node=2 train.py --cfg-path configs/train_stage2.yaml --resume_checkpoint outputs_stage2/202501290330/checkpoint_5.pth
W0130 02:34:50.321788 3076963 site-packages/torch/distributed/run.py:793] 
W0130 02:34:50.321788 3076963 site-packages/torch/distributed/run.py:793] *****************************************
W0130 02:34:50.321788 3076963 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0130 02:34:50.321788 3076963 site-packages/torch/distributed/run.py:793] *****************************************
| distributed init (rank 0, world 2): env://
| distributed init (rank 1, world 2): env://
[rank0]:[W130 02:34:56.436668013 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W130 02:34:56.437867922 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
0
1
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: sinseunghun708 (sinseunghun708-student). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/KJPark/audiolm-evaluator/audiolm-trainer/wandb/run-20250130_023457-h89a1crq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vicuna-7B-stage2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sinseunghun708-student/audio_lm
wandb: üöÄ View run at https://wandb.ai/sinseunghun708-student/audio_lm/runs/h89a1crq
2025-01-30 02:34:57,992 [INFO] 
=====  Running Parameters    =====
2025-01-30 02:34:57,993 [INFO] {
    "accum_grad_iters": 1,
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "epoch_based": false,
    "evaluate": false,
    "exp_name": "vicuna-7B-stage2",
    "gpu": 0,
    "iters_per_epoch": 3000,
    "log_freq": 5,
    "num_workers": 8,
    "optims": {
        "beta2": 0.999,
        "init_lr": 3e-05,
        "max_epoch": 30,
        "min_lr": 1e-05,
        "warmup_start_lr": 1e-06,
        "warmup_steps": 3000,
        "weight_decay": 0.05
    },
    "output_dir": "outputs_stage2",
    "rank": 0,
    "seed": 42,
    "use_distributed": true,
    "world_size": 2
}
2025-01-30 02:34:57,993 [INFO] 
======  Dataset Attributes  ======
2025-01-30 02:34:57,994 [INFO] {
    "prefix": "/data/data",
    "train_ann_path": "/data/data/stage2/stage2_train_modified.json",
    "valid_ann_path": "/data/data/stage2/stage2_valid_modified.json",
    "whisper_path": "openai/whisper-large-v2"
}
2025-01-30 02:34:57,994 [INFO] 
======  Model Attributes  ======
2025-01-30 02:34:57,994 [INFO] {
    "beats_path": "/data/baseline/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt",
    "ckpt": "/data/nathan/audiolm-evaluator/audiolm-trainer/outputs_stage1_only/202501272256/checkpoint_best.pth",
    "end_sym": "</s>",
    "freeze_beats": true,
    "freeze_speech_QFormer": false,
    "freeze_speech_llama_proj": false,
    "freeze_whisper": true,
    "llama_path": "lmsys/vicuna-7b-v1.5",
    "lora": true,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_rank": 8,
    "max_txt_len": 300,
    "multi_prompt": true,
    "num_speech_query_token": 1,
    "prompt_path": "prompts/train_prompt.json",
    "prompt_template": "USER: {}\nASSISTANT:",
    "second_per_window": 0.333333,
    "second_stride": 0.333333,
    "speech_llama_proj_model": "",
    "test_prompt_path": "prompts/test_prompt.json",
    "token": "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",
    "use_speech_Qformer": true,
    "whisper_path": "openai/whisper-large-v2",
    "window_level_Qformer": true
}
Loading checkpoint shards:   0%|                                                                                                                                      | 0/2 [00:00<?, ?it/s]2025-01-30 02:35:01,729 [INFO] Loading LLaMA Tokenizer
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 1/2 [00:00<00:00,  2.55it/s]2025-01-30 02:35:02,217 [INFO] Loading LLaMA Model
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.07it/s]
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.17it/s]
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2025-01-30 02:37:20,655 [INFO] Loading LLaMA Done
trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525
trainable params: 4194304 || all params: 6742618112 || trainable%: 0.06220586618327525
2025-01-30 02:37:38,156 [INFO] LoRA Training
2025-01-30 02:37:38,156 [INFO] Loading Whisper Model
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-01-30 02:37:39,256 [INFO] freeze Whisper
2025-01-30 02:37:39,256 [INFO] Loading BEATs Model
2025-01-30 02:37:39,563 [INFO] BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 0.6, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.0, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': True, 'predictor_dropout': 0.0, 'predictor_class': 527}
/data/miniconda3/envs/KJPark/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-30 02:37:42,317 [INFO] freeze BEATs
BertLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading training prompts done!
2025-01-30 02:37:43,934 [INFO] Loading speech LLAMA proj
Loading training prompts done!
2025-01-30 02:37:43,958 [INFO] Load SALMONN ckpt from: /data/nathan/audiolm-evaluator/audiolm-trainer/outputs_stage1_only/202501272256/checkpoint_best.pth
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
# LoRA Ï†ÅÏö©ÏùÑ ÏúÑÌï¥ Llama Î™®Îç∏Ïùò layer Î≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Î°úÎìú(layer Î≥Ñ 4Í∞ú - Q,V, Lora A, Lora B)
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.
# Í∞Å ÏùåÏÑ± Î∞è ÎπÑÏùåÏÑ± ÌäπÏßïÏóê ÎåÄÌï¥ Layer Normalization Ï†ÅÏö© -> Í∞Å ÌäπÏßï Î≥Ñ mean Î∞è stdÏùÑ Ï°∞Ï†ïÌïòÏó¨ Î™®Îç∏ ÏÑ±Îä• Ìñ•ÏÉÅ
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
GPU Memory Before Loading:
Allocated: 15925.00 MB
Cached: 31234.00 MB
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:82: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler()
module.speech_query_tokens
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location='cuda')
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
module.ln_speech.weight
module.ln_speech.bias
module.ln_audio.weight
module.ln_audio.bias
module.speech_Qformer.bert.embeddings.LayerNorm.weight
module.speech_Qformer.bert.embeddings.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.speech_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
module.speech_llama_proj.weight
module.speech_llama_proj.bias
2025-01-30 02:37:50,087 [INFO] number of trainable parameters: 30186240
GPU Memory Before Loading:
Allocated: 15925.00 MB
Cached: 31234.00 MB
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:406: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location='cuda')
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
2025-01-30 02:37:51,122 [INFO] Training Phase
2025-01-30 02:37:51,131 [INFO] Start training epoch 5, 3000 iters per inner epoch.
/data/KJPark/audiolm-evaluator/audiolm-trainer/runner.py:134: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=self.use_amp):
Train: data epoch: [5]  [   0/3000]  eta: 2:21:10  lr: 0.000029  loss: 0.0787  time: 2.8235  data: 0.0000  max mem: 25386
Train: data epoch: [5]  [   0/3000]  eta: 2:28:12  lr: 0.000029  loss: 0.0610  time: 2.9640  data: 0.0000  max mem: 20142
Train: data epoch: [5]  [   5/3000]  eta: 1:14:09  lr: 0.000029  loss: 0.1520  time: 1.4856  data: 0.0000  max mem: 25962
Train: data epoch: [5]  [   5/3000]  eta: 1:13:04  lr: 0.000029  loss: 0.0575  time: 1.4641  data: 0.0000  max mem: 25455
Train: data epoch: [5]  [  10/3000]  eta: 1:05:36  lr: 0.000029  loss: 1.1862  time: 1.3164  data: 0.0000  max mem: 25962
Train: data epoch: [5]  [  10/3000]  eta: 1:05:00  lr: 0.000029  loss: 0.1322  time: 1.3045  data: 0.0000  max mem: 25455
Train: data epoch: [5]  [  15/3000]  eta: 1:04:11  lr: 0.000029  loss: 0.1709  time: 1.2904  data: 0.